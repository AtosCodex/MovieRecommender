{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sh\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/22/17b22ef5b049f12080f5815c41bf94de3c229217609e469001a8f80c1b3d/sh-1.12.14-py2.py3-none-any.whl\n",
      "\u001b[31mgoogle-cloud-pubsub 0.26.0 has requirement google-cloud-core<0.26dev,>=0.25.0, but you'll have google-cloud-core 0.28.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorflow-transform 0.8.0 has requirement protobuf<4,>=3.6.0, but you'll have protobuf 3.5.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mgapic-google-cloud-pubsub-v1 0.15.4 has requirement oauth2client<4.0dev,>=2.0.0, but you'll have oauth2client 4.1.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mproto-google-cloud-pubsub-v1 0.15.4 has requirement oauth2client<4.0dev,>=2.0.0, but you'll have oauth2client 4.1.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mapache-airflow 1.9.0 has requirement bleach==2.1.2, but you'll have bleach 3.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mapache-airflow 1.9.0 has requirement jinja2<2.9.0,>=2.7.3, but you'll have jinja2 2.10 which is incompatible.\u001b[0m\n",
      "\u001b[31mapache-airflow 1.9.0 has requirement psutil<5.0.0,>=4.2.0, but you'll have psutil 5.4.7 which is incompatible.\u001b[0m\n",
      "\u001b[31mgoogle-cloud-bigquery 0.25.0 has requirement google-cloud-core<0.26dev,>=0.25.0, but you'll have google-cloud-core 0.28.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mgoogle-cloud-monitoring 0.28.0 has requirement google-api-core<0.2.0dev,>=0.1.1, but you'll have google-api-core 1.5.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mmltoolbox-datalab-image-classification 0.2 has requirement pillow==3.4.1, but you'll have pillow 5.3.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorboard 1.8.0 has requirement bleach==1.5.0, but you'll have bleach 3.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorboard 1.8.0 has requirement html5lib==0.9999999, but you'll have html5lib 1.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mproto-google-cloud-datastore-v1 0.90.0 has requirement oauth2client<4.0dev,>=2.0.0, but you'll have oauth2client 4.1.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mpandas-gbq 0.3.0 has requirement google-cloud-bigquery>=0.28.0, but you'll have google-cloud-bigquery 0.25.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mgoogledatastore 7.0.1 has requirement httplib2<0.10,>=0.9.1, but you'll have httplib2 0.11.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mgoogledatastore 7.0.1 has requirement oauth2client<4.0.0,>=2.0.1, but you'll have oauth2client 4.1.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: sh\n",
      "Successfully installed sh-1.12.14\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wals.py\n"
     ]
    }
   ],
   "source": [
    "%writefile wals.py\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.factorization.python.ops import factorization_ops\n",
    "\n",
    "\n",
    "def get_rmse(output_row, output_col, actual):\n",
    "  \"\"\"Compute rmse between predicted and actual ratings.\n",
    "\n",
    "  Args:\n",
    "    output_row: evaluated numpy array of row_factor\n",
    "    output_col: evaluated numpy array of col_factor\n",
    "    actual: coo_matrix of actual (test) values\n",
    "\n",
    "  Returns:\n",
    "    rmse\n",
    "  \"\"\"\n",
    "  mse = 0\n",
    "  for i in xrange(actual.data.shape[0]):\n",
    "    row_pred = output_row[actual.row[i]]\n",
    "    col_pred = output_col[actual.col[i]]\n",
    "    err = actual.data[i] - np.dot(row_pred, col_pred)\n",
    "    mse += err * err\n",
    "  mse /= actual.data.shape[0]\n",
    "  rmse = math.sqrt(mse)\n",
    "  return rmse\n",
    "\n",
    "\n",
    "def simple_train(model, input_tensor, num_iterations):\n",
    "  \"\"\"Helper function to train model on input for num_iterations.\n",
    "\n",
    "  Args:\n",
    "    model:            WALSModel instance\n",
    "    input_tensor:     SparseTensor for input ratings matrix\n",
    "    num_iterations:   number of row/column updates to run\n",
    "\n",
    "  Returns:\n",
    "    tensorflow session, for evaluating results\n",
    "  \"\"\"\n",
    "  sess = tf.Session(graph=input_tensor.graph)\n",
    "\n",
    "  with input_tensor.graph.as_default():\n",
    "    row_update_op = model.update_row_factors(sp_input=input_tensor)[1]\n",
    "    col_update_op = model.update_col_factors(sp_input=input_tensor)[1]\n",
    "\n",
    "    sess.run(model.initialize_op)\n",
    "    sess.run(model.worker_init)\n",
    "    for _ in xrange(num_iterations):\n",
    "      sess.run(model.row_update_prep_gramian_op)\n",
    "      sess.run(model.initialize_row_update_op)\n",
    "      sess.run(row_update_op)\n",
    "      sess.run(model.col_update_prep_gramian_op)\n",
    "      sess.run(model.initialize_col_update_op)\n",
    "      sess.run(col_update_op)\n",
    "\n",
    "  return sess\n",
    "\n",
    "LOG_RATINGS = 0\n",
    "LINEAR_RATINGS = 1\n",
    "LINEAR_OBS_W = 100.0\n",
    "\n",
    "\n",
    "def make_wts(data, wt_type, obs_wt, feature_wt_exp, axis):\n",
    "  \"\"\"Generate observed item weights.\n",
    "\n",
    "  Args:\n",
    "    data:             coo_matrix of ratings data\n",
    "    wt_type:          weight type, LOG_RATINGS or LINEAR_RATINGS\n",
    "    obs_wt:           linear weight factor\n",
    "    feature_wt_exp:   logarithmic weight factor\n",
    "    axis:             axis to make weights for, 1=rows/users, 0=cols/items\n",
    "\n",
    "  Returns:\n",
    "    vector of weights for cols (items) or rows (users)\n",
    "  \"\"\"\n",
    "  # recipricol of sum of number of items across rows (if axis is 0)\n",
    "  frac = np.array(1.0/(data > 0.0).sum(axis))\n",
    "\n",
    "  # filter any invalid entries\n",
    "  frac[np.ma.masked_invalid(frac).mask] = 0.0\n",
    "\n",
    "  # normalize weights according to assumed distribution of ratings\n",
    "  if wt_type == LOG_RATINGS:\n",
    "    wts = np.array(np.power(frac, feature_wt_exp)).flatten()\n",
    "  else:\n",
    "    wts = np.array(obs_wt * frac).flatten()\n",
    "\n",
    "  # check again for any numerically unstable entries\n",
    "  assert np.isfinite(wts).sum() == wts.shape[0]\n",
    "  return wts\n",
    "\n",
    "\n",
    "def wals_model(data, dim, reg, unobs, weights=False,\n",
    "               wt_type=LINEAR_RATINGS, feature_wt_exp=None,\n",
    "               obs_wt=LINEAR_OBS_W):\n",
    "  \"\"\"Create the WALSModel and input, row and col factor tensors.\n",
    "\n",
    "  Args:\n",
    "    data:           scipy coo_matrix of item ratings\n",
    "    dim:            number of latent factors\n",
    "    reg:            regularization constant\n",
    "    unobs:          unobserved item weight\n",
    "    weights:        True: set obs weights, False: obs weights = unobs weights\n",
    "    wt_type:        feature weight type: linear (0) or log (1)\n",
    "    feature_wt_exp: feature weight exponent constant\n",
    "    obs_wt:         feature weight linear factor constant\n",
    "\n",
    "  Returns:\n",
    "    input_tensor:   tensor holding the input ratings matrix\n",
    "    row_factor:     tensor for row_factor\n",
    "    col_factor:     tensor for col_factor\n",
    "    model:          WALSModel instance\n",
    "  \"\"\"\n",
    "  row_wts = None\n",
    "  col_wts = None\n",
    "\n",
    "  num_rows = data.shape[0]\n",
    "  num_cols = data.shape[1]\n",
    "\n",
    "  if weights:\n",
    "    assert feature_wt_exp is not None\n",
    "    row_wts = np.ones(num_rows)\n",
    "    col_wts = make_wts(data, wt_type, obs_wt, feature_wt_exp, 0)\n",
    "\n",
    "  row_factor = None\n",
    "  col_factor = None\n",
    "\n",
    "  with tf.Graph().as_default():\n",
    "\n",
    "    input_tensor = tf.SparseTensor(indices=zip(data.row, data.col),\n",
    "                                   values=(data.data).astype(np.float32),\n",
    "                                   dense_shape=data.shape)\n",
    "\n",
    "    model = factorization_ops.WALSModel(num_rows, num_cols, dim,\n",
    "                                        unobserved_weight=unobs,\n",
    "                                        regularization=reg,\n",
    "                                        row_weights=row_wts,\n",
    "                                        col_weights=col_wts)\n",
    "\n",
    "    # retrieve the row and column factors\n",
    "    row_factor = model.row_factors[0]\n",
    "    col_factor = model.col_factors[0]\n",
    "\n",
    "  return input_tensor, row_factor, col_factor, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix\n",
    "import sh\n",
    "import tensorflow as tf\n",
    "\n",
    "import wals\n",
    "\n",
    "# ratio of train set size to test set size\n",
    "TEST_SET_RATIO = 10\n",
    "\n",
    "# default hyperparameters\n",
    "DEFAULT_PARAMS = {\n",
    "    'weights': True,\n",
    "    'latent_factors': 5,\n",
    "    'num_iters': 20,\n",
    "    'regularization': 0.07,\n",
    "    'unobs_weight': 0.01,\n",
    "    'wt_type': 0,\n",
    "    'feature_wt_factor': 130.0,\n",
    "    'feature_wt_exp': 0.08,\n",
    "    'delimiter': '\\t'\n",
    "}\n",
    "\n",
    "# parameters optimized with hypertuning for the MovieLens data set\n",
    "OPTIMIZED_PARAMS = {\n",
    "    'latent_factors': 34,\n",
    "    'regularization': 9.83,\n",
    "    'unobs_weight': 0.001,\n",
    "    'feature_wt_factor': 189.8,\n",
    "}\n",
    "\n",
    "# parameters optimized with hypertuning for the included web views data set\n",
    "OPTIMIZED_PARAMS_WEB = {\n",
    "    'latent_factors': 30,\n",
    "    'regularization': 7.27,\n",
    "    'unobs_weight': 0.01,\n",
    "    'feature_wt_exp': 5.05,\n",
    "}\n",
    "\n",
    "\n",
    "def create_test_and_train_sets():\n",
    "  \n",
    "  return _ratings_train_and_test()\n",
    "\n",
    "def _ratings_train_and_test():\n",
    "  headers = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "  #header_row = 0 if use_headers else None\n",
    "  \n",
    "  ratings_df = pd.read_pickle(\"../data/rating.pkl\")\n",
    " # ratings_df = pd.read_csv(input_file,\n",
    " #                          sep=delimiter,\n",
    " #                          names=headers,\n",
    " #                          header=header_row,\n",
    " #                          dtype={\n",
    " #                              'user_id': np.int32,\n",
    " #                              'item_id': np.int32,\n",
    " #                              'rating': np.float32,\n",
    " #                              'timestamp': np.int32,\n",
    " #                          })\n",
    "\n",
    "  np_users = ratings_df.userId.as_matrix()\n",
    "  np_items = ratings_df.movieId.as_matrix()\n",
    "  unique_users = np.unique(np_users)\n",
    "  unique_items = np.unique(np_items)\n",
    "\n",
    "  n_users = unique_users.shape[0]\n",
    "  n_items = unique_items.shape[0]\n",
    "\n",
    "  # make indexes for users and items if necessary\n",
    "  max_user = unique_users[-1]\n",
    "  max_item = unique_items[-1]\n",
    "  if n_users != max_user or n_items != max_item:\n",
    "    # make an array of 0-indexed unique user ids corresponding to the dataset\n",
    "    # stack of user ids\n",
    "    z = np.zeros(max_user+1, dtype=int)\n",
    "    z[unique_users] = np.arange(n_users)\n",
    "    u_r = z[np_users]\n",
    "\n",
    "    # make an array of 0-indexed unique item ids corresponding to the dataset\n",
    "    # stack of item ids\n",
    "    z = np.zeros(max_item+1, dtype=int)\n",
    "    z[unique_items] = np.arange(n_items)\n",
    "    i_r = z[np_items]\n",
    "\n",
    "    # construct the ratings set from the three stacks\n",
    "    np_ratings = ratings_df.rating.as_matrix()\n",
    "    ratings = np.zeros((np_ratings.shape[0], 3), dtype=object)\n",
    "    ratings[:, 0] = u_r\n",
    "    ratings[:, 1] = i_r\n",
    "    ratings[:, 2] = np_ratings\n",
    "  else:\n",
    "    ratings = ratings_df.as_matrix(['userId', 'movieId', 'rating'])\n",
    "    # deal with 1-based user indices\n",
    "    ratings[:, 0] -= 1\n",
    "    ratings[:, 1] -= 1\n",
    "\n",
    "  tr_sparse, test_sparse = _create_sparse_train_and_test(ratings,\n",
    "                                                         n_users, n_items)\n",
    "\n",
    "  return ratings[:, 0], ratings[:, 1], tr_sparse, test_sparse\n",
    "\n",
    "def _create_sparse_train_and_test(ratings, n_users, n_items):\n",
    "  \"\"\"Given ratings, create sparse matrices for train and test sets.\n",
    "\n",
    "  Args:\n",
    "    ratings:  list of ratings tuples  (u, i, r)\n",
    "    n_users:  number of users\n",
    "    n_items:  number of items\n",
    "\n",
    "  Returns:\n",
    "     train, test sparse matrices in scipy coo_matrix format.\n",
    "  \"\"\"\n",
    "  # pick a random test set of entries, sorted ascending\n",
    "  test_set_size = len(ratings) / TEST_SET_RATIO\n",
    "  test_set_idx = np.random.choice(xrange(len(ratings)),\n",
    "                                  size=test_set_size, replace=False)\n",
    "  test_set_idx = sorted(test_set_idx)\n",
    "\n",
    "  # sift ratings into train and test sets\n",
    "  ts_ratings = ratings[test_set_idx]\n",
    "  tr_ratings = np.delete(ratings, test_set_idx, axis=0)\n",
    "\n",
    "  # create training and test matrices as coo_matrix's\n",
    "  u_tr, i_tr, r_tr = zip(*tr_ratings)\n",
    "  tr_sparse = coo_matrix((r_tr, (u_tr, i_tr)), shape=(n_users, n_items))\n",
    "\n",
    "  u_ts, i_ts, r_ts = zip(*ts_ratings)\n",
    "  test_sparse = coo_matrix((r_ts, (u_ts, i_ts)), shape=(n_users, n_items))\n",
    "\n",
    "  return tr_sparse, test_sparse\n",
    "\n",
    "\n",
    "def train_model(args, tr_sparse):\n",
    "  dim = args['latent_factors']\n",
    "  num_iters = args['num_iters']\n",
    "  reg = args['regularization']\n",
    "  unobs = args['unobs_weight']\n",
    "  wt_type = args['wt_type']\n",
    "  feature_wt_exp = args['feature_wt_exp']\n",
    "  obs_wt = args['feature_wt_factor']\n",
    "\n",
    "  tf.logging.info('Train Start: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "\n",
    "  # generate model\n",
    "  input_tensor, row_factor, col_factor, model = wals.wals_model(tr_sparse,\n",
    "                                                                dim,\n",
    "                                                                reg,\n",
    "                                                                unobs,\n",
    "                                                                args['weights'],\n",
    "                                                                wt_type,\n",
    "                                                                feature_wt_exp,\n",
    "                                                                obs_wt)\n",
    "\n",
    "  # factorize matrix\n",
    "  session = wals.simple_train(model, input_tensor, num_iters)\n",
    "\n",
    "  tf.logging.info('Train Finish: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "\n",
    "  # evaluate output factor matrices\n",
    "  output_row = row_factor.eval(session=session)\n",
    "  output_col = col_factor.eval(session=session)\n",
    "\n",
    "  # close the training session now that we've evaluated the output\n",
    "  session.close()\n",
    "\n",
    "  return output_row, output_col\n",
    "\n",
    "\n",
    "def save_model(output_dir, user_map, item_map, row_factor, col_factor):\n",
    "  \"\"\"Save the user map, item map, row factor and column factor matrices in numpy format.\n",
    "\n",
    "  These matrices together constitute the \"recommendation model.\"\n",
    "\n",
    "  Args:\n",
    "    args:         input args to training job\n",
    "    user_map:     user map numpy array\n",
    "    item_map:     item map numpy array\n",
    "    row_factor:   row_factor numpy array\n",
    "    col_factor:   col_factor numpy array\n",
    "  \"\"\"\n",
    "  model_dir = os.path.join(output_dir, 'model')\n",
    "\n",
    "  # if our output directory is a GCS bucket, write model files to /tmp,\n",
    "  # then copy to GCS\n",
    "  gs_model_dir = None\n",
    "  if model_dir.startswith('gs://'):\n",
    "    gs_model_dir = model_dir\n",
    "    model_dir = '/tmp/{0}'.format(args['job_name'])\n",
    "\n",
    "  os.makedirs(model_dir)\n",
    "  np.save(os.path.join(model_dir, 'user'), user_map)\n",
    "  np.save(os.path.join(model_dir, 'item'), item_map)\n",
    "  np.save(os.path.join(model_dir, 'row'), row_factor)\n",
    "  np.save(os.path.join(model_dir, 'col'), col_factor)\n",
    "\n",
    "  if gs_model_dir:\n",
    "    sh.gsutil('cp', '-r', os.path.join(model_dir, '*'), gs_model_dir)\n",
    "\n",
    "\n",
    "def generate_recommendations(user_idx, user_rated, row_factor, col_factor, k):\n",
    "  \"\"\"Generate recommendations for a user.\n",
    "\n",
    "  Args:\n",
    "    user_idx: the row index of the user in the ratings matrix,\n",
    "\n",
    "    user_rated: the list of item indexes (column indexes in the ratings matrix)\n",
    "      previously rated by that user (which will be excluded from the\n",
    "      recommendations)\n",
    "\n",
    "    row_factor: the row factors of the recommendation model\n",
    "    col_factor: the column factors of the recommendation model\n",
    "\n",
    "    k: number of recommendations requested\n",
    "\n",
    "  Returns:\n",
    "    list of k item indexes with the predicted highest rating, excluding\n",
    "    those that the user has already rated\n",
    "  \"\"\"\n",
    "\n",
    "  # bounds checking for args\n",
    "  assert (row_factor.shape[0] - len(user_rated)) >= k\n",
    "\n",
    "  # retrieve user factor\n",
    "  user_f = row_factor[user_idx]\n",
    "\n",
    "  # dot product of item factors with user factor gives predicted ratings\n",
    "  pred_ratings = col_factor.dot(user_f)\n",
    "\n",
    "  # find candidate recommended item indexes sorted by predicted rating\n",
    "  k_r = k + len(user_rated)\n",
    "  candidate_items = np.argsort(pred_ratings)[-k_r:]\n",
    "\n",
    "  # remove previously rated items and take top k\n",
    "  recommended_items = [i for i in candidate_items if i not in user_rated]\n",
    "  recommended_items = recommended_items[-k:]\n",
    "\n",
    "  # flip to sort highest rated first\n",
    "  recommended_items.reverse()\n",
    "\n",
    "  return recommended_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Train Start: 2018-11-15 00:01:52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wals.py:78: RuntimeWarning: divide by zero encountered in divide\n",
      "  frac = np.array(1.0/(data > 0.0).sum(axis))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Train Finish: 2018-11-15 00:10:35\n"
     ]
    }
   ],
   "source": [
    "user_map, item_map, tr_sparse, test_sparse = create_test_and_train_sets()\n",
    "\n",
    "# train model\n",
    "output_row, output_col = train_model(DEFAULT_PARAMS,tr_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08462361, -0.00880599, -0.12086756, -0.05155351,  0.00057303],\n",
       "       [ 0.01294671, -0.013276  , -0.13062036, -0.07056771,  0.06378343],\n",
       "       [ 0.01071315, -0.06041512, -0.1710389 , -0.11614319,  0.07454851],\n",
       "       [-0.04837557,  0.07757223,  0.02727203, -0.09036379,  0.05118196],\n",
       "       [-0.0612067 ,  0.05296154,  0.01687476, -0.15498507,  0.05184331],\n",
       "       [-0.0480038 ,  0.05832568,  0.04342399, -0.09595051,  0.05410079],\n",
       "       [ 0.04157277, -0.03180509, -0.16222945, -0.03692691,  0.12430366],\n",
       "       [-0.05904764,  0.07055259,  0.04166314, -0.12034764,  0.06223192],\n",
       "       [ 0.01364668,  0.00810396, -0.12072343, -0.04024697, -0.03066311],\n",
       "       [-0.01774213, -0.06533276, -0.04875777, -0.13858376, -0.00329655]],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.4422902e+01,  1.0490119e+01, -3.4396281e+00, -2.9438894e+01,\n",
       "         1.2982123e+01],\n",
       "       [ 8.9928703e+00,  1.3298990e+01, -2.9393589e+00, -1.9970472e+01,\n",
       "         1.1342815e+01],\n",
       "       [ 6.2391849e+00,  1.3416862e+01, -2.7574584e+00, -1.5324193e+01,\n",
       "         1.2780069e+01],\n",
       "       [ 2.9167972e+00,  6.9432101e+00, -1.7683459e-02, -5.2384443e+00,\n",
       "         1.3022038e+01],\n",
       "       [ 5.2895060e+00,  1.3856513e+01, -2.5004339e+00, -1.4203545e+01,\n",
       "         1.2202555e+01],\n",
       "       [ 1.4257142e+01,  1.0314872e+01, -1.3568208e+00, -2.6551668e+01,\n",
       "         1.1500297e+01],\n",
       "       [ 6.9345126e+00,  1.2044910e+01, -2.2734742e+00, -1.6261740e+01,\n",
       "         1.4841030e+01],\n",
       "       [ 2.7945809e+00,  8.7452698e+00, -3.2205217e+00, -3.0813365e+00,\n",
       "         1.1338194e+01],\n",
       "       [ 1.5278209e+00,  1.2065713e+01, -6.2334198e-01, -7.0377474e+00,\n",
       "         1.2271125e+01],\n",
       "       [ 1.0681404e+01,  1.3159863e+01, -3.5619824e+00, -2.2896538e+01,\n",
       "         1.1112011e+01]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(output_row[0:10])\n",
    "display(output_col[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model('wals_out', user_map, item_map, output_row, output_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 1.27013637583 / Test RMSE: 1.2963350423\n"
     ]
    }
   ],
   "source": [
    "# log results\n",
    "train_rmse = wals.get_rmse(output_row, output_col, tr_sparse)\n",
    "test_rmse = wals.get_rmse(output_row, output_col, test_sparse)\n",
    "print('Train RMSE: %s / Test RMSE: %s' % (train_rmse,test_rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
