{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sh\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/22/17b22ef5b049f12080f5815c41bf94de3c229217609e469001a8f80c1b3d/sh-1.12.14-py2.py3-none-any.whl\n",
      "\u001b[31mgoogle-cloud-pubsub 0.26.0 has requirement google-cloud-core<0.26dev,>=0.25.0, but you'll have google-cloud-core 0.28.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorflow-transform 0.8.0 has requirement protobuf<4,>=3.6.0, but you'll have protobuf 3.5.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mgapic-google-cloud-pubsub-v1 0.15.4 has requirement oauth2client<4.0dev,>=2.0.0, but you'll have oauth2client 4.1.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mproto-google-cloud-pubsub-v1 0.15.4 has requirement oauth2client<4.0dev,>=2.0.0, but you'll have oauth2client 4.1.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mapache-airflow 1.9.0 has requirement bleach==2.1.2, but you'll have bleach 3.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mapache-airflow 1.9.0 has requirement jinja2<2.9.0,>=2.7.3, but you'll have jinja2 2.10 which is incompatible.\u001b[0m\n",
      "\u001b[31mapache-airflow 1.9.0 has requirement psutil<5.0.0,>=4.2.0, but you'll have psutil 5.4.7 which is incompatible.\u001b[0m\n",
      "\u001b[31mgoogle-cloud-bigquery 0.25.0 has requirement google-cloud-core<0.26dev,>=0.25.0, but you'll have google-cloud-core 0.28.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mgoogle-cloud-monitoring 0.28.0 has requirement google-api-core<0.2.0dev,>=0.1.1, but you'll have google-api-core 1.5.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mmltoolbox-datalab-image-classification 0.2 has requirement pillow==3.4.1, but you'll have pillow 5.3.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorboard 1.8.0 has requirement bleach==1.5.0, but you'll have bleach 3.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorboard 1.8.0 has requirement html5lib==0.9999999, but you'll have html5lib 1.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mproto-google-cloud-datastore-v1 0.90.0 has requirement oauth2client<4.0dev,>=2.0.0, but you'll have oauth2client 4.1.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mpandas-gbq 0.3.0 has requirement google-cloud-bigquery>=0.28.0, but you'll have google-cloud-bigquery 0.25.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mgoogledatastore 7.0.1 has requirement httplib2<0.10,>=0.9.1, but you'll have httplib2 0.11.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mgoogledatastore 7.0.1 has requirement oauth2client<4.0.0,>=2.0.1, but you'll have oauth2client 4.1.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: sh\n",
      "Successfully installed sh-1.12.14\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wals.py\n"
     ]
    }
   ],
   "source": [
    "%writefile wals.py\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.factorization.python.ops import factorization_ops\n",
    "\n",
    "\n",
    "def get_rmse(output_row, output_col, actual):\n",
    "  \"\"\"Compute rmse between predicted and actual ratings.\n",
    "\n",
    "  Args:\n",
    "    output_row: evaluated numpy array of row_factor\n",
    "    output_col: evaluated numpy array of col_factor\n",
    "    actual: coo_matrix of actual (test) values\n",
    "\n",
    "  Returns:\n",
    "    rmse\n",
    "  \"\"\"\n",
    "  mse = 0\n",
    "  for i in xrange(actual.data.shape[0]):\n",
    "    row_pred = output_row[actual.row[i]]\n",
    "    col_pred = output_col[actual.col[i]]\n",
    "    err = actual.data[i] - np.dot(row_pred, col_pred)\n",
    "    mse += err * err\n",
    "  mse /= actual.data.shape[0]\n",
    "  rmse = math.sqrt(mse)\n",
    "  return rmse\n",
    "\n",
    "\n",
    "def simple_train(model, input_tensor, num_iterations):\n",
    "  \"\"\"Helper function to train model on input for num_iterations.\n",
    "\n",
    "  Args:\n",
    "    model:            WALSModel instance\n",
    "    input_tensor:     SparseTensor for input ratings matrix\n",
    "    num_iterations:   number of row/column updates to run\n",
    "\n",
    "  Returns:\n",
    "    tensorflow session, for evaluating results\n",
    "  \"\"\"\n",
    "  sess = tf.Session(graph=input_tensor.graph)\n",
    "\n",
    "  with input_tensor.graph.as_default():\n",
    "    row_update_op = model.update_row_factors(sp_input=input_tensor)[1]\n",
    "    col_update_op = model.update_col_factors(sp_input=input_tensor)[1]\n",
    "\n",
    "    sess.run(model.initialize_op)\n",
    "    sess.run(model.worker_init)\n",
    "    for _ in xrange(num_iterations):\n",
    "      sess.run(model.row_update_prep_gramian_op)\n",
    "      sess.run(model.initialize_row_update_op)\n",
    "      sess.run(row_update_op)\n",
    "      sess.run(model.col_update_prep_gramian_op)\n",
    "      sess.run(model.initialize_col_update_op)\n",
    "      sess.run(col_update_op)\n",
    "\n",
    "  return sess\n",
    "\n",
    "LOG_RATINGS = 0\n",
    "LINEAR_RATINGS = 1\n",
    "LINEAR_OBS_W = 100.0\n",
    "\n",
    "\n",
    "def make_wts(data, wt_type, obs_wt, feature_wt_exp, axis):\n",
    "  \"\"\"Generate observed item weights.\n",
    "\n",
    "  Args:\n",
    "    data:             coo_matrix of ratings data\n",
    "    wt_type:          weight type, LOG_RATINGS or LINEAR_RATINGS\n",
    "    obs_wt:           linear weight factor\n",
    "    feature_wt_exp:   logarithmic weight factor\n",
    "    axis:             axis to make weights for, 1=rows/users, 0=cols/items\n",
    "\n",
    "  Returns:\n",
    "    vector of weights for cols (items) or rows (users)\n",
    "  \"\"\"\n",
    "  # recipricol of sum of number of items across rows (if axis is 0)\n",
    "  frac = np.array(1.0/(data > 0.0).sum(axis))\n",
    "\n",
    "  # filter any invalid entries\n",
    "  frac[np.ma.masked_invalid(frac).mask] = 0.0\n",
    "\n",
    "  # normalize weights according to assumed distribution of ratings\n",
    "  if wt_type == LOG_RATINGS:\n",
    "    wts = np.array(np.power(frac, feature_wt_exp)).flatten()\n",
    "  else:\n",
    "    wts = np.array(obs_wt * frac).flatten()\n",
    "\n",
    "  # check again for any numerically unstable entries\n",
    "  assert np.isfinite(wts).sum() == wts.shape[0]\n",
    "  return wts\n",
    "\n",
    "\n",
    "def wals_model(data, dim, reg, unobs, weights=False,\n",
    "               wt_type=LINEAR_RATINGS, feature_wt_exp=None,\n",
    "               obs_wt=LINEAR_OBS_W):\n",
    "  \"\"\"Create the WALSModel and input, row and col factor tensors.\n",
    "\n",
    "  Args:\n",
    "    data:           scipy coo_matrix of item ratings\n",
    "    dim:            number of latent factors\n",
    "    reg:            regularization constant\n",
    "    unobs:          unobserved item weight\n",
    "    weights:        True: set obs weights, False: obs weights = unobs weights\n",
    "    wt_type:        feature weight type: linear (0) or log (1)\n",
    "    feature_wt_exp: feature weight exponent constant\n",
    "    obs_wt:         feature weight linear factor constant\n",
    "\n",
    "  Returns:\n",
    "    input_tensor:   tensor holding the input ratings matrix\n",
    "    row_factor:     tensor for row_factor\n",
    "    col_factor:     tensor for col_factor\n",
    "    model:          WALSModel instance\n",
    "  \"\"\"\n",
    "  row_wts = None\n",
    "  col_wts = None\n",
    "\n",
    "  num_rows = data.shape[0]\n",
    "  num_cols = data.shape[1]\n",
    "\n",
    "  if weights:\n",
    "    assert feature_wt_exp is not None\n",
    "    row_wts = np.ones(num_rows)\n",
    "    col_wts = make_wts(data, wt_type, obs_wt, feature_wt_exp, 0)\n",
    "\n",
    "  row_factor = None\n",
    "  col_factor = None\n",
    "\n",
    "  with tf.Graph().as_default():\n",
    "\n",
    "    input_tensor = tf.SparseTensor(indices=zip(data.row, data.col),\n",
    "                                   values=(data.data).astype(np.float32),\n",
    "                                   dense_shape=data.shape)\n",
    "\n",
    "    model = factorization_ops.WALSModel(num_rows, num_cols, dim,\n",
    "                                        unobserved_weight=unobs,\n",
    "                                        regularization=reg,\n",
    "                                        row_weights=row_wts,\n",
    "                                        col_weights=col_wts)\n",
    "\n",
    "    # retrieve the row and column factors\n",
    "    row_factor = model.row_factors[0]\n",
    "    col_factor = model.col_factors[0]\n",
    "\n",
    "  return input_tensor, row_factor, col_factor, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix\n",
    "import sh\n",
    "import tensorflow as tf\n",
    "\n",
    "import wals\n",
    "\n",
    "# ratio of train set size to test set size\n",
    "TEST_SET_RATIO = 10\n",
    "\n",
    "# default hyperparameters\n",
    "DEFAULT_PARAMS = {\n",
    "    'weights': True,\n",
    "    'latent_factors': 5,\n",
    "    'num_iters': 20,\n",
    "    'regularization': 0.07,\n",
    "    'unobs_weight': 0.01,\n",
    "    'wt_type': 0,\n",
    "    'feature_wt_factor': 130.0,\n",
    "    'feature_wt_exp': 0.08,\n",
    "    'delimiter': '\\t'\n",
    "}\n",
    "\n",
    "# parameters optimized with hypertuning for the MovieLens data set\n",
    "OPTIMIZED_PARAMS = {\n",
    "    'latent_factors': 34,\n",
    "    'regularization': 9.83,\n",
    "    'unobs_weight': 0.001,\n",
    "    'feature_wt_factor': 189.8,\n",
    "}\n",
    "\n",
    "# parameters optimized with hypertuning for the included web views data set\n",
    "OPTIMIZED_PARAMS_WEB = {\n",
    "    'latent_factors': 30,\n",
    "    'regularization': 7.27,\n",
    "    'unobs_weight': 0.01,\n",
    "    'feature_wt_exp': 5.05,\n",
    "}\n",
    "\n",
    "\n",
    "def create_test_and_train_sets():\n",
    "  \n",
    "  return _ratings_train_and_test()\n",
    "\n",
    "def _ratings_train_and_test():\n",
    "  headers = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "  #header_row = 0 if use_headers else None\n",
    "  \n",
    "  ratings_df = pd.read_pickle(\"../data/rating.pkl\")\n",
    " # ratings_df = pd.read_csv(input_file,\n",
    " #                          sep=delimiter,\n",
    " #                          names=headers,\n",
    " #                          header=header_row,\n",
    " #                          dtype={\n",
    " #                              'user_id': np.int32,\n",
    " #                              'item_id': np.int32,\n",
    " #                              'rating': np.float32,\n",
    " #                              'timestamp': np.int32,\n",
    " #                          })\n",
    "\n",
    "  np_users = ratings_df.userId.as_matrix()\n",
    "  np_items = ratings_df.movieId.as_matrix()\n",
    "  unique_users = np.unique(np_users)\n",
    "  unique_items = np.unique(np_items)\n",
    "\n",
    "  n_users = unique_users.shape[0]\n",
    "  n_items = unique_items.shape[0]\n",
    "\n",
    "  # make indexes for users and items if necessary\n",
    "  max_user = unique_users[-1]\n",
    "  max_item = unique_items[-1]\n",
    "  if n_users != max_user or n_items != max_item:\n",
    "    # make an array of 0-indexed unique user ids corresponding to the dataset\n",
    "    # stack of user ids\n",
    "    z = np.zeros(max_user+1, dtype=int)\n",
    "    z[unique_users] = np.arange(n_users)\n",
    "    u_r = z[np_users]\n",
    "\n",
    "    # make an array of 0-indexed unique item ids corresponding to the dataset\n",
    "    # stack of item ids\n",
    "    z = np.zeros(max_item+1, dtype=int)\n",
    "    z[unique_items] = np.arange(n_items)\n",
    "    i_r = z[np_items]\n",
    "\n",
    "    # construct the ratings set from the three stacks\n",
    "    np_ratings = ratings_df.rating.as_matrix()\n",
    "    ratings = np.zeros((np_ratings.shape[0], 3), dtype=object)\n",
    "    ratings[:, 0] = u_r\n",
    "    ratings[:, 1] = i_r\n",
    "    ratings[:, 2] = np_ratings\n",
    "  else:\n",
    "    ratings = ratings_df.as_matrix(['userId', 'movieId', 'rating'])\n",
    "    # deal with 1-based user indices\n",
    "    ratings[:, 0] -= 1\n",
    "    ratings[:, 1] -= 1\n",
    "\n",
    "  tr_sparse, test_sparse = _create_sparse_train_and_test(ratings,\n",
    "                                                         n_users, n_items)\n",
    "\n",
    "  return ratings[:, 0], ratings[:, 1], tr_sparse, test_sparse\n",
    "\n",
    "def _create_sparse_train_and_test(ratings, n_users, n_items):\n",
    "  \"\"\"Given ratings, create sparse matrices for train and test sets.\n",
    "\n",
    "  Args:\n",
    "    ratings:  list of ratings tuples  (u, i, r)\n",
    "    n_users:  number of users\n",
    "    n_items:  number of items\n",
    "\n",
    "  Returns:\n",
    "     train, test sparse matrices in scipy coo_matrix format.\n",
    "  \"\"\"\n",
    "  # pick a random test set of entries, sorted ascending\n",
    "  test_set_size = len(ratings) / TEST_SET_RATIO\n",
    "  test_set_idx = np.random.choice(xrange(len(ratings)),\n",
    "                                  size=test_set_size, replace=False)\n",
    "  test_set_idx = sorted(test_set_idx)\n",
    "\n",
    "  # sift ratings into train and test sets\n",
    "  ts_ratings = ratings[test_set_idx]\n",
    "  tr_ratings = np.delete(ratings, test_set_idx, axis=0)\n",
    "\n",
    "  # create training and test matrices as coo_matrix's\n",
    "  u_tr, i_tr, r_tr = zip(*tr_ratings)\n",
    "  tr_sparse = coo_matrix((r_tr, (u_tr, i_tr)), shape=(n_users, n_items))\n",
    "\n",
    "  u_ts, i_ts, r_ts = zip(*ts_ratings)\n",
    "  test_sparse = coo_matrix((r_ts, (u_ts, i_ts)), shape=(n_users, n_items))\n",
    "\n",
    "  return tr_sparse, test_sparse\n",
    "\n",
    "\n",
    "def train_model(args, tr_sparse):\n",
    "  dim = args['latent_factors']\n",
    "  num_iters = args['num_iters']\n",
    "  reg = args['regularization']\n",
    "  unobs = args['unobs_weight']\n",
    "  wt_type = args['wt_type']\n",
    "  feature_wt_exp = args['feature_wt_exp']\n",
    "  obs_wt = args['feature_wt_factor']\n",
    "\n",
    "  tf.logging.info('Train Start: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "\n",
    "  # generate model\n",
    "  input_tensor, row_factor, col_factor, model = wals.wals_model(tr_sparse,\n",
    "                                                                dim,\n",
    "                                                                reg,\n",
    "                                                                unobs,\n",
    "                                                                args['weights'],\n",
    "                                                                wt_type,\n",
    "                                                                feature_wt_exp,\n",
    "                                                                obs_wt)\n",
    "\n",
    "  # factorize matrix\n",
    "  session = wals.simple_train(model, input_tensor, num_iters)\n",
    "\n",
    "  tf.logging.info('Train Finish: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "\n",
    "  # evaluate output factor matrices\n",
    "  output_row = row_factor.eval(session=session)\n",
    "  output_col = col_factor.eval(session=session)\n",
    "\n",
    "  # close the training session now that we've evaluated the output\n",
    "  session.close()\n",
    "\n",
    "  return output_row, output_col, model\n",
    "\n",
    "\n",
    "def save_model(output_dir, user_map, item_map, row_factor, col_factor):\n",
    "  \"\"\"Save the user map, item map, row factor and column factor matrices in numpy format.\n",
    "\n",
    "  These matrices together constitute the \"recommendation model.\"\n",
    "\n",
    "  Args:\n",
    "    args:         input args to training job\n",
    "    user_map:     user map numpy array\n",
    "    item_map:     item map numpy array\n",
    "    row_factor:   row_factor numpy array\n",
    "    col_factor:   col_factor numpy array\n",
    "  \"\"\"\n",
    "  model_dir = os.path.join(output_dir, 'model')\n",
    "\n",
    "  # if our output directory is a GCS bucket, write model files to /tmp,\n",
    "  # then copy to GCS\n",
    "  gs_model_dir = None\n",
    "  if model_dir.startswith('gs://'):\n",
    "    gs_model_dir = model_dir\n",
    "    model_dir = '/tmp/{0}'.format(args['job_name'])\n",
    "\n",
    "  os.makedirs(model_dir)\n",
    "  np.save(os.path.join(model_dir, 'user'), user_map)\n",
    "  np.save(os.path.join(model_dir, 'item'), item_map)\n",
    "  np.save(os.path.join(model_dir, 'row'), row_factor)\n",
    "  np.save(os.path.join(model_dir, 'col'), col_factor)\n",
    "\n",
    "  if gs_model_dir:\n",
    "    sh.gsutil('cp', '-r', os.path.join(model_dir, '*'), gs_model_dir)\n",
    "\n",
    "\n",
    "def generate_recommendations(user_idx, user_rated, row_factor, col_factor, k):\n",
    "  \"\"\"Generate recommendations for a user.\n",
    "\n",
    "  Args:\n",
    "    user_idx: the row index of the user in the ratings matrix,\n",
    "\n",
    "    user_rated: the list of item indexes (column indexes in the ratings matrix)\n",
    "      previously rated by that user (which will be excluded from the\n",
    "      recommendations)\n",
    "\n",
    "    row_factor: the row factors of the recommendation model\n",
    "    col_factor: the column factors of the recommendation model\n",
    "\n",
    "    k: number of recommendations requested\n",
    "\n",
    "  Returns:\n",
    "    list of k item indexes with the predicted highest rating, excluding\n",
    "    those that the user has already rated\n",
    "  \"\"\"\n",
    "\n",
    "  # bounds checking for args\n",
    "  assert (row_factor.shape[0] - len(user_rated)) >= k\n",
    "\n",
    "  # retrieve user factor\n",
    "  user_f = row_factor[user_idx]\n",
    "\n",
    "  # dot product of item factors with user factor gives predicted ratings\n",
    "  pred_ratings = col_factor.dot(user_f)\n",
    "\n",
    "  # find candidate recommended item indexes sorted by predicted rating\n",
    "  k_r = k + len(user_rated)\n",
    "  candidate_items = np.argsort(pred_ratings)[-k_r:]\n",
    "\n",
    "  # remove previously rated items and take top k\n",
    "  recommended_items = [i for i in candidate_items if i not in user_rated]\n",
    "  recommended_items = recommended_items[-k:]\n",
    "\n",
    "  # flip to sort highest rated first\n",
    "  recommended_items.reverse()\n",
    "\n",
    "  return recommended_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Train Start: 2018-11-15 15:07:37\n",
      "INFO:tensorflow:Train Finish: 2018-11-15 15:16:18\n"
     ]
    }
   ],
   "source": [
    "user_map, item_map, tr_sparse, test_sparse = create_test_and_train_sets()\n",
    "\n",
    "# train model\n",
    "output_row, output_col, md = train_model(DEFAULT_PARAMS,tr_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "row  = np.array([0, 0, 0, 0])\n",
    "col  = np.array([50, 153, 671, 1002])\n",
    "data = np.array([4.5, 5.0, 4.5, 4.9], dtype=np.float32)\n",
    "new_user = coo_matrix(data, (row, col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'project_row_factors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-283283b1614c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                                    \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                    dense_shape=new_user.shape)\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnew_user_embd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject_row_factors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_user_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'project_row_factors'"
     ]
    }
   ],
   "source": [
    "new_user_tensor = tf.SparseTensor(indices=zip(new_user.row, new_user.col),\n",
    "                                   values=(new_user.data).astype(np.float32),\n",
    "                                   dense_shape=new_user.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_embds = output_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = KDTree(movies_embds, leaf_size=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_representation = sum([movies_embds[i] for i in [140, 340, 5002]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8.8227005, -20.886433 ,   6.42346  ,  12.361511 ,  -3.4397814],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11.313215 , -23.997679 ,   8.53643  ,  13.413322 ,   5.4243336]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_embds[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, recs = tree.query(user_representation.reshape(1, -1), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter and the Chamber of Secrets (2002)\n",
      "Lost in Space (1998)\n",
      "Monty Python and the Holy Grail (1975)\n",
      "RECOMMENDED:\n",
      "1375    Last of the Mohicans, The (1992)\n",
      "1934                        Bambi (1942)\n",
      "120                     Boomerang (1992)\n",
      "721                     Rock, The (1996)\n",
      "793              American Buffalo (1996)\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for i in [140, 340, 5002]:\n",
    "  print(df_movies.iloc[item_map[i]].title)\n",
    "print('RECOMMENDED:')\n",
    "for i in recs:\n",
    "  print(df_movies.iloc[item_map[i]].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08462361, -0.00880599, -0.12086756, -0.05155351,  0.00057303],\n",
       "       [ 0.01294671, -0.013276  , -0.13062036, -0.07056771,  0.06378343],\n",
       "       [ 0.01071315, -0.06041512, -0.1710389 , -0.11614319,  0.07454851],\n",
       "       [-0.04837557,  0.07757223,  0.02727203, -0.09036379,  0.05118196],\n",
       "       [-0.0612067 ,  0.05296154,  0.01687476, -0.15498507,  0.05184331]],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.4422902e+01,  1.0490119e+01, -3.4396281e+00, -2.9438894e+01,\n",
       "         1.2982123e+01],\n",
       "       [ 8.9928703e+00,  1.3298990e+01, -2.9393589e+00, -1.9970472e+01,\n",
       "         1.1342815e+01],\n",
       "       [ 6.2391849e+00,  1.3416862e+01, -2.7574584e+00, -1.5324193e+01,\n",
       "         1.2780069e+01],\n",
       "       [ 2.9167972e+00,  6.9432101e+00, -1.7683459e-02, -5.2384443e+00,\n",
       "         1.3022038e+01],\n",
       "       [ 5.2895060e+00,  1.3856513e+01, -2.5004339e+00, -1.4203545e+01,\n",
       "         1.2202555e+01]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(output_row[0:5])\n",
    "display(output_col[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model('wals_out', user_map, item_map, output_row, output_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 1.27013637583 / Test RMSE: 1.2963350423\n"
     ]
    }
   ],
   "source": [
    "# log results\n",
    "train_rmse = wals.get_rmse(output_row, output_col, tr_sparse)\n",
    "test_rmse = wals.get_rmse(output_row, output_col, test_sparse)\n",
    "print('Train RMSE: %s / Test RMSE: %s' % (train_rmse,test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(user_map[600:650])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 28, 31, 46, 49, 110, 149, 220, 250, 257, 290, 293, 315, 333,\n",
       "       363, 537, 583, 587, 645, 902, 907, 990, 1017, 1057, 1058, 1067,\n",
       "       1068, 1075, 1113, 1169, 1171, 1173, 1175, 1176, 1182, 1188, 1189,\n",
       "       1191, 1193, 1196, 1212, 1215, 1218, 1221, 1230, 1231, 1233, 1234,\n",
       "       1238, 1250], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(item_map[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138493"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26744"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000263"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000263"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(item_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve user factor\n",
    "user_f = output_row[6]\n",
    "len(user_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8595\n"
     ]
    }
   ],
   "source": [
    "user_idx = np.searchsorted(user_map,77)\n",
    "print(user_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot product of item factors with user factor gives predicted ratings\n",
    "pred_ratings = output_col.dot(user_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26744"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(len(pred_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/ipykernel/__main__.py:237: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    }
   ],
   "source": [
    "recommendations = generate_recommendations(user_idx, np.array(['1']),\n",
    "                                                 output_row,\n",
    "                                                 output_col,\n",
    "                                                 6)\n",
    "\n",
    "# map article indexes back to article ids\n",
    "article_recommendations = [item_map[i] for i in recommendations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2597, 352, 1839, 2602, 3707, 3157]\n"
     ]
    }
   ],
   "source": [
    "print(article_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = pd.read_pickle(\"../data/movies.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>movie_year</th>\n",
       "      <th>(no genres listed)</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Children</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>...</th>\n",
       "      <th>Film-Noir</th>\n",
       "      <th>Horror</th>\n",
       "      <th>IMAX</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>352</td>\n",
       "      <td>Crooklyn (1994)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>1839</td>\n",
       "      <td>My Giant (1998)</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>2597</td>\n",
       "      <td>Lost &amp; Found (1999)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517</th>\n",
       "      <td>2602</td>\n",
       "      <td>Mighty Peking Man (a.k.a. Goliathon) (Xing xin...</td>\n",
       "      <td>Action|Adventure|Horror|Sci-Fi</td>\n",
       "      <td>1977.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3070</th>\n",
       "      <td>3157</td>\n",
       "      <td>Stuart Little (1999)</td>\n",
       "      <td>Children|Comedy|Fantasy</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3616</th>\n",
       "      <td>3707</td>\n",
       "      <td>9 1/2 Weeks (Nine 1/2 Weeks) (1986)</td>\n",
       "      <td>Drama|Romance</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      movieId                                              title  \\\n",
       "348       352                                    Crooklyn (1994)   \n",
       "1757     1839                                    My Giant (1998)   \n",
       "2512     2597                                Lost & Found (1999)   \n",
       "2517     2602  Mighty Peking Man (a.k.a. Goliathon) (Xing xin...   \n",
       "3070     3157                               Stuart Little (1999)   \n",
       "3616     3707                9 1/2 Weeks (Nine 1/2 Weeks) (1986)   \n",
       "\n",
       "                              genres  movie_year  (no genres listed)  Action  \\\n",
       "348                     Comedy|Drama      1994.0                   0       0   \n",
       "1757                          Comedy      1998.0                   0       0   \n",
       "2512                  Comedy|Romance      1999.0                   0       0   \n",
       "2517  Action|Adventure|Horror|Sci-Fi      1977.0                   0       1   \n",
       "3070         Children|Comedy|Fantasy      1999.0                   0       0   \n",
       "3616                   Drama|Romance      1986.0                   0       0   \n",
       "\n",
       "      Adventure  Animation  Children  Comedy   ...     Film-Noir  Horror  \\\n",
       "348           0          0         0       1   ...             0       0   \n",
       "1757          0          0         0       1   ...             0       0   \n",
       "2512          0          0         0       1   ...             0       0   \n",
       "2517          1          0         0       0   ...             0       1   \n",
       "3070          0          0         1       1   ...             0       0   \n",
       "3616          0          0         0       0   ...             0       0   \n",
       "\n",
       "      IMAX  Musical  Mystery  Romance  Sci-Fi  Thriller  War  Western  \n",
       "348      0        0        0        0       0         0    0        0  \n",
       "1757     0        0        0        0       0         0    0        0  \n",
       "2512     0        0        0        1       0         0    0        0  \n",
       "2517     0        0        0        0       1         0    0        0  \n",
       "3070     0        0        0        0       0         0    0        0  \n",
       "3616     0        0        0        1       0         0    0        0  \n",
       "\n",
       "[6 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_movies[df_movies['movieId'].isin(article_recommendations)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
